---
title: "Laboratory 4"
author: "Emily Tipton"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: yes
---
# Tasks

# Task 1
```{r}
getwd()
```

## Task 2

```{r}
spruce <- read.csv("SPRUCE.csv")
tail(spruce)
```

## Task 3

### Create a scatter plot

```{r}
library(s20x)

trendscatter(data = spruce, Height~BHDiameter, f = 0.5, main = "f=0.5")
```

### Make a linear model object

```{r}
spruce_lm=with(spruce,lm(Height~BHDiameter))
```

### Find residuals

```{r}
height_res = residuals(spruce_lm)
height_res
```

### Find fitted values

```{r}
height_fit = fitted(spruce_lm)
height_fit
```

### Plot residuals vs fitted values

```{r}
plot(height_fit, height_res, xlab = "Fitted values", ylab = "Residuals", main = "Residuals vs Fitted values", pch =21, bg = "pink", cex = 1.2)
```

### Plot residuals vs fitted values using trendscatter()

```{r}
trendscatter(data = spruce_lm, height_res~height_fit, f = 0.5, main = "Residuals vs Fitted Values", xlab = "Fitted values", ylab = "Residuals")
```

### Describe the plot and compare

The residuals vs fitted values trendscatter has a negative quadratic shape. There's a peak near the middle. The first trendscatter with the Height and BHDiameter data doesn't have a dramatic decrease after the peak. 

### Make the residual plot using spruce_lm

```{r}
plot(spruce_lm, which = 1, pch = 21, bg = "black")
```

### Check normality

```{r}
normcheck(spruce_lm, shapiro.wilk = TRUE)
```

### P-value and NULL hypothesis

P-value = 0.29
If the p-value is < 0.05, we reject the NULL hypothesis. In this case, since our p-value = 0.29 > 0.05, we will accept the NULL hypothesis.

### Conclusions

Although we accept the NULL hypothesis based on the p-value, there's evidence to show that the straight line model is not the best fit for our data. The residuals vs fitted values plot show a quadratic curve, meaning a straight line model wouldn't be an appropriate choice. There's signal in the test when it should be noise, furthering the point that a straight line is not the best fit.

## Task 4

### Fit a quadratic

```{r}
quad_lm = lm(Height~BHDiameter + I(BHDiameter ^2), data=spruce)

summary(quad_lm)
```

### Scatter plot with quadratic curve

```{r}
coef(quad_lm)
```


```{r}
plot(spruce, main = "Height vs BHDiameter", pch =21, bg = "black")

quadplot = function(x) {
  quad_lm$coefficients[1] + quad_lm$coefficients[2] * x + quad_lm$coefficients[3] * x ^2
}

curve(quadplot, lwd = 2, col = "deeppink3", add = TRUE)
```
 
### Vector of fitted values

```{r}
quad_fit = fitted(quad_lm)
```

### Residuals vs Fitted values from quad_lm

```{r}
plot(quad_lm, which = 1, pch = 21, bg = "black")
```

### Check normality

```{r}
normcheck(quad_lm, shapiro.wilk = TRUE)
```

### Conclusions

The P-value is 0.684 which is larger than the P-value from our previous Shapiro-Wilk normality check. This means that the quadratic curve that we fit to our data is more accurate than the straight line. There is significantly less signal which is what we were aiming for. The shape from our normality check is much more symmetrical. 

## Task 5

### Summarize quad_lm

```{r}
summary(quad_lm)
```

### Estimate values

$\hat{\beta_{0}}$ = 0.860896

$\hat{\beta_{1}}$ = 1.469592

$\hat{\beta_{2}}$ = -0.027457

### Interval estimates

```{r}
confint(quad_lm)
```

### Equation of fitted line


$$ f(x) = 0.860896 + 1.469592x - 0.027457x^2 $$

### Predict height of spruce

diameter = 15, 18, and 20 cm

```{r}
predict(quad_lm, data.frame(BHDiameter=c(15,18,20)))
```

### Previous predictions

```{r}
predict(spruce_lm, data.frame(BHDiameter=c(15, 18, 20)))
```

```{r}
with(spruce,
     plot(Height~BHDiameter, data = spruce, main = "Height vs BHDiameter", ylab = "Height (m)", xlab = "Breast Height Diameter (cm)", pch =21, bg = "blue", cex=1.2, xlim = c(0, max(BHDiameter)*1.1), ylim = c(0, max(Height) * 1.1))
)

abline(spruce_lm)
```
 
```{r}
plot(spruce, main = "Height vs BHDiameter", pch =21, bg = "black")

quadplot = function(x) {
  quad_lm$coefficients[1] + quad_lm$coefficients[2] * x + quad_lm$coefficients[3] * x ^2
}

curve(quadplot, lwd = 2, col = "deeppink3", add = TRUE)
```

By observing the two plots visually, we can see that the straight line is lower than our data points whereas the quadratic line is much closer to the data points on the plots.
This is represented numerically with the predict function.
The quadratic fit is more accurate to the data points than the straight line.

### Value of multiple $R^2$

```{r}
summary(quad_lm)
```

According to the summary table, multiple $R^2$ = 0.7741

```{r}
summary(spruce_lm)
```

Our previous multiple $R^2$ value was 0.6569 

The value of $R^2$ in the current model is greater than the previous one. 

### Adjusted $R^2$

The adjusted $R^2$ value for the quad fit is 0.7604 and 0.6468 for the linear fit. It's beneficial to use an adjusted $R^2$ value because it provides a more precise observation of the correlation. The adjusted $R^2$ value takes into account the amount of independent variables that are added to the model. 

### (multiple $R^2$)

In Lab 3 and 4, we've created regression models in order to predict the value of the Height of Spruce given a certain BHDiameter. We used a straight line fit for our first model and a quadratic fit for the second one. The conditions we set for each regression model contribute to the accuracy of our predictions. The value of multiple $R^2$ allows us to see how useful the predictor variables(specifically the straight line fit vs the quadratic fit) are at calculating the prediction of the Height.

By calculating the proportion of the total number of squares explained by the model (MSS/TSS), we can see how many residuals are present in the model. As the $R^2$ value approaches 1, the number of residuals is decreased. In the case of our models, the $R^2$ value was greater for the quadratic fit, which means we have less residuals. This means that the quadratic fit is more accurate in predicting the Height than the linear fit. 

### Variability in Height

The quadratic model explains the most variability in Height. This can be seen in the increase of the $R^2$ value from linear to quadratic. This value is determinant in how well the variation of BHDiameter explains the variation of Height. The larger the $R^2$ value, the better the model is at predicting the variation in Y based on the variation in X.

### anova

```{r}
anova(spruce_lm)
anova(quad_lm)
```

The anova table shows that the amount of residuals decreased when we modeled the data with a quadratic fit rather than the linear fit. This is important because the lower number of residuals means that the fit is more accurate for predictions. 

### RSS, MSS, TSS, MSS / TSS

```{r}
RSS=with(spruce, sum((Height-quad_fit)^2))
RSS
```
```{r}
MSS = with(spruce, sum((quad_fit-mean(Height))^2))
MSS
```

```{r}
TSS = with(spruce, sum((Height-mean(Height))^2))
TSS
```

```{r}
MSS / TSS
```

$R^2$ = MSS / TSS

## Task 6

### Unusual points

```{r}
library(s20x)
cooks20x(quad_lm)
```

### Use of Cook's Distance

A Cook's distance plot is used to show the influence of data points on the fitted values. The value of the distance takes into account both the Height and BHDiameter values. The longer the distance, the more influential the point is to the fit.  

### Cook's with quadratic model

In our case, the data point labelled 24 is very influential because it has the largest Cook's distance. The two points with the second and third most influence are points 18 and 21. This means that these particular points are skewing the data away from the fit. By removing these influential points, we can create a more accurate fit. 

### quad2_lm with removed data points

```{r}
quad2_lm = lm(Height~BHDiameter + I(BHDiameter ^2), data=spruce[-c(18,21,24),])
quad2_lm
```

### Summarize

```{r}
summary(quad2_lm)
```

### Compare

```{r}
summary(quad_lm)
```

### Conclusions

The residual standard error measures how well the regression model fits the data. The lower the value, the better the fit. In our case, the residual standard error is lower with quad2_lm than quad1_lm. I noticed that removing the data points with the highest Cook's distance changed the intercept from a positive value to negative. 

## Task 7

### Proof

Suppose line 1 and line 2 are written as:

$$ L1: y = \beta_{0} + \beta_{1} x $$
$$ L2: y = \beta_{0} + \delta + (\beta_{1} + \zeta)x $$

The intersection of the two lines (the change point in the piecewise regression) would then be:

$$ \beta_{0} + \beta_{1} x_{k} = \beta_{0} + \delta + (\beta_{1} + \zeta) x_{k} $$

By reducing like terms, we have:

$$ \delta = -\zeta x_{k} $$

Substituting $\delta$ in L2:

$$ y = \beta_{0} -\zeta x_{k} + (\beta_{1} + \zeta)x $$

Rearranging

$$ y = \beta_{0} + \beta_{1}x +\zeta (x - x_{k}) $$

Therefore, L2 = L1 + adjustment term. 
This implies that

$$ y = \beta_{0} + \beta_{1}x +\zeta (x - x_{k})I(x > x_{k}) $$
where I(x > $x_{k}$) = 1 when x > $x_{k}$ and 0 otherwise.

### Recreate plot

```{r}
spruce2 =within(spruce, X<-(BHDiameter-20)*(BHDiameter>20))
spruce2
```

```{r}
lmp=lm(Height~BHDiameter + X,data=spruce2)
tmp=summary(lmp)
names(tmp)
```
```{r}
myf = function(x,coef){
  coef[1]+coef[2]*(x) + coef[3]*(x-18)*(x-18>0)
}
plot(spruce,main="Piecewise regression")
myf(0, coef=tmp$coefficients[,"Estimate"])
curve(myf(x,coef=tmp$coefficients[,"Estimate"] ),add=TRUE, lwd=2,col="deeppink3")
abline(v=18)
text(18,16,paste("R sq.=",round(tmp$r.squared,4) ))
```

## Task 8

### Load package
```{r}
library(MATH4753tipt0010)
```

### Use function

```{r}
z(1:5)
```

### Explain

The function I created takes a vector and returns the z values. The output is a list of the created z values and the given x values.










